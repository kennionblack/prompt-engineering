# Skynet

## Brief description

This project creates an agentic chatbot interface that can dynamically build, execute, and evaluate code within subagents to perform tasks requested by the user.

These code snippets (with associated READMEs) are stored in `/agent/skills` subdirectories with dedicated subagents that perform tasks specific to the skill domain to reduce context drift in the main user interface agent. Agents are able to write their own skill code and execute this code in a sandboxed Docker environment. When a session is ended, the generated skill code and README are uploaded to S3. Similarly, when a new session is started, all skills present in S3 are read from S3 and copied down to the user's local file system such that the skills and associated subagents can be used in different contexts.

## Worklog

Work completed each week is recorded in the associated worklog for each subdirectory, but I have composited the worklog entries related to this project [here](https://github.com/kennionblack/prompt-engineering/tree/main/skynet/composite_worklog.md).

## Design documents

As part of the planning process for this bot, I made a sequence diagram for how I wanted the flow of this project to work. Some of the orchestration present in the sequence diagram is not in the actual finished product, but the majority of the functionality demonstrated in the diagram is working inside the bot.

[Diagram link](https://github.com/kennionblack/prompt-engineering/tree/main/skynet/sequence_diagram.svg)

## How to actually run this

This project has both a CLI mode and a web interface mode that can be used to interact with the agent. This can be initialized by navigating to the root of the `skynet` directory and running `python3 agent.py` for the interactive CLI mode (which comes with a lot of helpful debugging output) or `python3 agent.py --web` to initialize a web interface for the chatbot. This project also requires a `.env` file structured as shown below:

```
OPENAI_API_KEY=your-api-key-here
AWS_S3_BUCKET_NAME=your-bucket-name-here
AWS_ACCESS_KEY=your-access-key-here
AWS_SECRET_ACCESS_KEY=your-secret-access-key-here
```

Note that it should be possible to execute the program without including S3 credentials, but this also comes with no guarantee of persistence. The IAM user associated with the access key and secret access key will need access to upload, download, list, and modify files in S3. If you are unsure how to get AWS access keys, consult the official AWS documentation [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

## Presentation

I presented on this project with a demo [here](https://byu.sharepoint.com/sites/CS452Fall2025/_layouts/15/stream.aspx?id=%2Fsites%2FCS452Fall2025%2FShared%20Documents%2FClass%20and%20Recordings%2FRecordings%2FClass%20452%2D20251208%5F205327UTC%2DMeeting%20Recording%2Emp4&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview%2Ee5d9de85%2D61a5%2D40b0%2Db2c3%2D203ed23ba2d6) on 12/8/25. Apologies if you are not able to access the recording.

## AI integration

The backbone of this project was building dynamically generated AI agents. For a more detailed discussion of how I learned how to build these agents, consult the worklogs in the other directories.

## AI use

From what I'm seeing from the industry at the moment, it is in my best interest to learn how to use code assistance tools such as Claude and Copilot effectively as a performance multiplier (but not as a replacement for actual coding). As such, I extensively used Claude models with Copilot to generate large portions of the code. In general, I found these models to be beneficial in bringing my ideas to life. I chose to implement quality control by reading all of the changes generated by the model after each modification and ensuring that I understand the purpose and nuances of the code before further iteration. I do wish that I had used Git more liberally to save my work at certain points as there were a number of instances where proposed LLM edits were more far-reaching than intended and required significant work to revert.

## Why is this interesting

I've been fascinated by the idea of LLMs extending their own functionality dynamically for a while now and this project feels like a first step in this direction. One thing that I like about my approach is that the user is very much in the loop on what skills and knowledge should be accessible beyond a base level of LLM knowledge. I also like that users can write their own skill code within this framework as this allows individuals with domain-specific knowledge to "plug in" their knowledge to agents seamlessly.

## Key learnings

I learned a lot of things while building this project related to building effective agent frameworks and reliably executing AI-generated code.

1. When possible, enforce framework structure via code and not just in the system prompt for an agent. Agents are inherently nondeterministic, meaning that they will sometimes be able to complete a task in a way acceptable to a user and other times fail at the same task with the exact same prompt input. As such, I found that building scaffolding for the AI to write and execute its own code was often necessary to get any degree of reliability.
2. Static analysis of code helps fill in some of the gaps in AI-generated code. I built this system to be able to write and execute skill code for users that are not necessarily experienced coders, meaning that agents often needed to fill in a lot of context when generating skills. This lack of context meant that the generated code often had bugs when executed in a sandbox environment. To resolve this issue, I dug into static analysis methods available for Python code to verify that generated code would compile within the sandbox before execution as well as verify that types and docstrings were present in every function such that subagents would know how to invoke skill functions.
3. Agents perform best with focused tasks that are well described. One of the main challenges in effective AI usage these days is context drift, which can be understood as overloading an agent's message history with information unrelated to the core purpose of a conversation. By default, agents do not know how to prioritize specific portions of a conversation history over other less relevant portions, meaning that long-lived conversations start to degrade in quality (especially with complex tasks that require more context and user feedback). As such, subdividing tasks into smaller, focused tasks that can be executed outside of the main user interaction loop reduces the amount of extraneous information in agent context. I originally built this system to just have one agent that can write and execute skill code for itself, but sandbox retries and execution errors filled up the message history and pulled the agent off track. This new subagent paradigm for domain-specific knowledge performs much more consistently and also allows for async execution of multiple parallel agents in the future.

## Scaling, concurrency, etc.

This project was designed to be run locally on a user's computer, meaning that there isn't much scaling to be done on this project unless I decide to host the agent somewhere. I could see a limited version of concurrency being implemented in the future with a shared skill repository, but that is not implemented for the time being.

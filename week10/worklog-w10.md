# Week 10: Yet more iteration on the self-learning bot

## Executive Summary

This week I started to let other people play around with my agents, which led to finding some bugs and determining real use cases. I also (finally) fixed the hallucination issues plaguing previous attempts by prompting a different model to initiate a conversation, then switching to the original model after the user provides a response.

## Worklog

<!-- Note that this "table" was mostly generated with a VSCode extension that attempts to make all markdown table cells the same size, hence the strange formatting. I strongly recommend reading this report with something that actually renders the markdown instead of attempting to parse this mess visually. -->

| Date     | Time | Description                                                                                                                                                                                                                                        |
| -------- | ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 10/10/25 | 1h   | Attended lecture                                                                                                                                                                                                                                   |
| 10/11/25 | 1h   | Read up on agent orchestration strategies                                                                                                                                                                                                          |
| 10/12/25 | 1h   | Attended lecture                                                                                                                                                                                                                                   |
| 10/12/25 | 1h   | Run "stress tests" on bot with classmate prompts<br>Discover context window size limit issues                                                                                                                                                      |
| 10/13/25 | 4h   | Finally quash initial hallucination issue<br>Work around context window size limit with summarization subagent<br>Test context window limits with word counter tool for the Bible<br>Get network fetch summarization tools generating autonomously |
| 10/14/25 | 1h   | Strip out a lot of dead code from the agent framework that was no longer in use                                                                                                                                                                    |

## Exercises

This week I finally managed to find a consistent solution for the ![hallucination issues](https://github.com/kennionblack/prompt-engineering/tree/main/week8/worklog-w8.md) documented in [past worklogs](https://github.com/kennionblack/prompt-engineering/tree/main/week9/worklog-w9.md). I tracked down the issue to the reasoning capabilities of `gpt-5-mini` that decided to bootstrap their own context (seemingly from training data), which led me to try using `gpt-4o`. This worked flawlessly, but I still wanted to retain some of the reasoning capabilities of `gpt-5x` models (especially for code generation) so I fed the system prompt for the main user interface agent into a separate `client.responses.create()` call that used `gpt-4o`, then copied the history generated by the initial agent and client response into a `client.responses.create()` loop running `gpt-5-mini`. `gpt-4o` almost always generates the same response currently, but the agent is instructed to indicate some of its capabilities in the initial prompt. As such, I'd argue that the chat generation for the conversation starter is still more user-friendly than a hard-coded opening message.

I also had a classmate in another class use my agent framework. They were able to get it working with relatively minimal effort and found a bug with an AI-generated web fetch tool that returned raw HTML larger than the context window size for `gpt-5-mini` (128k tokens), which crashed the entire system. This led me to think about how to manage what AI-generated tools should return. Instead of adding instructions to the skill builder prompt, I decided to write a wrapper function for all tool responses that checks the size of the response and the remaining space in the agent context window, then summarizes content as needed to fit the response into the context window. I tested this functionality with the entirety of the Bible and found it to work seamlessly.

# Week 12: The Home Stretch

## Executive Summary

This week marked the final push to get my semester project presentation-ready. Major accomplishments included implementing library installation within sandboxes via AST parsing, building a chunking mechanism with summarization for large outputs, establishing proper agent orchestration with dynamic subagent creation, deploying a Gradio web interface, and implementing S3-based skill persistence with hash-based change detection. I also added comprehensive validation using static analysis to catch errors before sandbox execution, preventing the majority of runtime failures. The system now handles complex workflows including web scraping, skill creation, and cross-session persistence.

## Worklog

<!-- Note that this "table" was mostly generated with a VSCode extension that attempts to make all markdown table cells the same size, hence the strange formatting. I strongly recommend reading this report with something that actually renders the markdown instead of attempting to parse this mess visually. -->

| Date    | Time | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------- | ---- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 12/1/25 | 1h   | Attended lecture                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 12/2/25 | 2.5h | Figure out library installation within sandbox<br>Extensive testing with trying to get a web fetch skill working                                                                                                                                                                                                                                                                                                                                                                                     |
| 12/3/25 | 1h   | Attended lecture                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| 12/3/25 | 2h   | Figure out chunking mechanism for sandbox results exceeding context window size<br>Split out skill manager file into multiple modules for readability<br>Terminate about 30 concurrent Docker sessions leftover from uncleaned sandbox execution environments                                                                                                                                                                                                                                        |
| 12/4/25 | 3h   | Move from executing skill code with the `user_interface` agent to dynamically creating subagents for each skill<br>Troubleshoot agent orchestration issues<br>Write wrapper script for sandbox code execution to ensure parseability<br>Get tool reloading working properly with new subagent paradigm                                                                                                                                                                                               |
| 12/5/25 | 3h   | Get a Gradio web interface for the agent up and running<br>Start working on an S3 sync process to persist skills across sessions<br>Handle type conversion from JSON-parseable types to native Python types (e.g. tuples)<br>Fix library detection issues for aliased imports                                                                                                                                                                                                                        |
| 12/6/25 | 5h   | Debug more agent orchestration issues<br>Add a validation function using static analysis to all code generated by LLM as sandbox execution kept failing<br>Remove tool duplication issue<br>Clear agent history across session reloads<br>Compute hash of skill functions to compare against stored version in S3 such that skills can be updated without deletion<br>Fix circular dependency issues<br>Build better chunking mechanism that summarizes each chunk before feeding result to subagent |

## Class notes

- Waystar CEO Matthew Hawkins visit
  - Church is using AI for (near) real-time translation of tribal languages in Africa for family history stories
  - Waystar - healthcare software to track provider bills, insurance, etc.
    - Centralizes payment orchestration
    - Implements AI usage/proficiency incentives and required AI certifications
  - How to use AI for moral good
    - MIT study: many organizations are trying to incorporate AI, but few get material benefits at the moment
    - LLMs used for automation of repetitive tasks
    - GenAI has historically not been integrated into business
      - Necessitates governance/steering
    - AI trained only on approved training data -- client data is encrypted and not used for training
    - Integrating people is a requirement, not a barrier
      - AI lacks empathy
    - Determine use cases to evaluate impact
    - Be wary of AI replacing humans for a profit margin as cost can increase unexpectedly
    - Questions to consider:
      - How to demnstrate data privacy, security, and fairness
      - How to measure adoption success
      - Is output auditable and defensible
      - What is the framework for monitoring and training
      - What role does a company have in training clients on appropriate use of AI
      - How to maintain human involvement and oversight
      - What tasks can be wholly outsourced to AI and what needs human input
      - Does data housed in an LLM become a target
    - Data purity and fair data use will likely become the defining factor in effective AI
    - Most AI legislation is only at the state level for now

## Exercises

This week was spent getting my semester project to a state where it would actually be presentable. I'll list out some of the main challenges from this week's work below:

| **Issue**                                                                       | **Solution**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| External library imports were not available in the sandbox                      | Iterate through the AST of the agent-generated code to extract modules, then `pip install` libraries as part of sandbox initialization process. This had to account for module level imports, specific function imports, function aliasing, and PyPI package names not matching the name imported via module.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Sandbox output exceeded context window size                                     | This was a common issue when attempting to build an agent capable of fetching information from the Internet. Even with instructions in the system prompt for the web fetch agent to keep result sizes small, the scraped results returned from most webpages easily blew past the context window size of the subagent. My solution to this was to build a chunking mechanism that detects outputs greater than ~8000 tokens, then passes chunks of ~6000 to a separate agent that summarizes the content of each chunk and aggregates the result into something smaller than the token limit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Delegated subagents don't know when to transfer control to another agent        | Despite adding instructions to subagent system prompts to route all user communication through the `user_interface` agent, most subagents would attempt to send responses to the user. I tried resolving this by only exposing the `talk_to_user` tool to the `user_interface` agent, but subagents were still somehow managing to bypass the tool and send messages to the user via the user interface. This poses an issue as subagents only have access to tools relevant to their associated skill, meaning that if the user responded to a delegated agent requesting functionality outside of the scope of its assigned skill it would hallucinate responses or outright refuse. I eventually found a workaround to this behavior by adding a check to all agent responses that forced the subagent message to be a return message unless the `talk_to_user` tool was accessible to the agent. Because subagent delegation is exposed via tool in this orchestration strategy, forcing a return effectively caused the `user_interface` agent to resume control with whatever information that the subagent provided. |
| AI generated code requires reruns to execute at all                             | Code generated by the AI was often buggy and would fail completely when run inside the sandbox environment. Instead of using yet another agent to evaluate code quality, I used static analysis methods to validate that the generated code contained information required for proper sandbox execution. If this validation check passed, I then called `ast.parse()` on the generated code, which attempts to compile the entire module generated by the agent. Any error from the static analysis or compilation were pushed onto a list of errors that was fed back to the `skill_builder` agent such that it could modify the output as needed. I placed this validation function into the `write_file` and `modify_file` tools used by the `skill_builder` agent to ensure that all .py files would be executable within the associated skill sandbox.                                                                                                                                                                                                                                                                 |
| Sandbox output is not able to be parsed by LLM due to not being JSON compatible | Generated skill code would occasionally use native Python return types that could not be parsed into JSON, meaning that the output of sandbox code was not able to be parsed by agents. I first added a wrapper script for all code executed in the sandbox that called `json.dumps()` on all output, but this would fail with complex types. I eventually wrote a `make_serializable()` function that handles byte, dict, and list types and falls back to the string representation of an object such that the result would always be serializable.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Skills can be updated locally, but not in S3                                    | It is possible for a skill to be updated in a local session, but my initial S3 check only verified that a skill existed matching the name used locally. This meant that the first draft of a skill would essentially be the version downloaded across sessions, overwriting local modifications. To avoid this, I compare the SHA hash of all of the local skill files to the skill files hosted in S3 to determine if files have been updated. If there are changes to a skill, the program gives the user the option to overwrite the skill version in S3, not upload their skill to S3, or only delete skills removed locally.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |

## Wish list

In no particular order, below are some features that I want to add to this project at some point:

- Make subagent execution async on separate threads to improve execution times
- Create code sandboxes in conjunction with the skill being created instead of needing to create a sandbox on the fly the first time a skill is invoked
- Add a cleanup process for orphaned or unused sandboxes
- Actually host the agent network somewhere
- User account management
- Better skill persistence than an S3 bucket
- Cleaner UI
- Actual orchestration layer for agents such that everything does not need to get routed through the user interface agent

The full-text talks embedding vector sets should be strongest at keeping overarching themes of a talk, especially when these themes span multiple paragraphs. In theory, this should help contextual information from being lost when a talk is divided into paragraphs, but in practice these vector sets have consistently lower similarity scores than the other vector groupings. This is likely due to thematic elements being diluted by tangential themes, which is not necessarily what a user would want with a targeted query.

The paragraph level embedding vector sets consistently produced the highest similarity scores. This is likely the "sweet spot" for semantic search for general conference talks as paragraphs are already an indicator of groupings of similar thoughts. However, because each vector is computed independently of other paragraphs in a talk, it is possible that the vector may de-emphasize themes that are repeated through a talk. This model is also weaker when speakers break up their statements into a high number of paragraphs (looking at you, Dieter F. Uchtdorf).

The clustered paragraph embedding vectors approach is an attempt at solving the thematic grouping issue from the pure paragraph vector approach. If a theme is prominent across multiple paragraphs, then the k-cluster process should help determine three aggregate vectors for a talk that separate themes out. However, because cluster size is not necessarily consistent, it is possible that one cluster with a high number of paragraph vectors can fall victim to the same dilution issue prevalent in the full text approach. In practice, the similarity scores for this approach were highly variable, suggesting that artificial grouping of three k-clusters can improve some queries on multi-thematic talks, but ultimately this approach may be inconsistent.

These themes were largely consistent between both the free and OpenAI embedding vectors. Interestingly, the similarity scores for the talk-level embedding vectors were noticeably higher for the free model versus the OpenAI model. I also noticed that the similarity scores for the clustered vectors were quite similar across both providers, which suggests that k-clustering creates some kind of normalization on the data.